---
title: "web_scraping"
author: "Olivia Abbott"
date: "10/24/2019"
output: html_document
---

```{r, results='hide', message=FALSE}
include <- function(library_name){
  if( !(library_name %in% installed.packages()) )
    install.packages(library_name) 
  library(library_name, character.only=TRUE)
}
include("tidyverse")
include("rvest")
include("tidyr")
```

To get the data from the website, I first used read html to read in the website data. Then to parse the data from the website I used html_nodes() and html_text().  Html_nodes() get the particular data from the selector that is passed into the funtion.  Html_text takes that information and converts it to text.  I did this with each piece of information that we wanted to get from the website (subject, catalog number, section, title, instructor, and total enrolled)

```{r}
class_html <- read_html("http://ems.csuchico.edu/APSS/schedule/spr2019/CSCI.shtml")

classes <- class_html %>% html_nodes("#maintable") 

subject <- classes %>% 
              html_nodes("td.subj") %>%
              html_text()

catalog_number <- classes %>% 
              html_nodes("td.cat_num") %>%
              html_text() %>%
              as.integer()

section <- classes %>% 
              html_nodes("td.sect") %>%
              html_text() %>%
              as.integer()

title <- classes %>% 
              html_nodes("td.title") %>%
              html_text()

instructor <- classes %>% 
              html_nodes("td.Instructor") %>%
              html_text()

total_enrolled <- classes %>% 
              html_nodes("td.enrtot") %>%
              html_text() %>%
              as.integer()

schedule <- tibble(subject = subject, catalog_number = catalog_number, section = section, title = title, instructor = instructor, total_enrolled = total_enrolled)
```

In order to do what was accomplished above with multiple websites, I simply took my previous code and put it within a function.  The function takes the website url that you would like to scrape and returns a tibble(table) with all the desired information.

After calling the function on the four websites that I wanted to scrape, I used rbind() to combine the four tables returned from the function into one table.  Rbind() simply appended each table at the bottom of the previous table since they all had the same columns.

```{r}
read_class_schedule <- function(url){
  class_html <- read_html(url)
  classes <- class_html %>% html_nodes("#maintable")
  subject <- classes %>% 
              html_nodes("td.subj") %>%
              html_text()
  catalog_number <- classes %>% 
              html_nodes("td.cat_num") %>%
              html_text() %>%
              as.integer()
  section <- classes %>% 
              html_nodes("td.sect") %>%
              html_text() %>%
              as.integer()
  title <- classes %>% 
              html_nodes("td.title") %>%
              html_text()
  instructor <- classes %>% 
              html_nodes("td.Instructor") %>%
              html_text()
  total_enrolled <- classes %>% 
              html_nodes("td.enrtot") %>%
              html_text() %>%
              as.integer()
  return(tibble(subject = subject, catalog_number = catalog_number, section = section, title = title, instructor = instructor, total_enrolled = total_enrolled))
}

csci_spring_2019 <- read_class_schedule("http://ems.csuchico.edu/APSS/schedule/spr2019/CSCI.shtml")
csci_spring_2020 <- read_class_schedule("http://ems.csuchico.edu/APSS/schedule/spr2020/CSCI.shtml")
math_spring_2019 <- read_class_schedule("http://ems.csuchico.edu/APSS/schedule/spr2019/MATH.shtml")
math_spring_2020 <- read_class_schedule("http://ems.csuchico.edu/APSS/schedule/spr2020/MATH.shtml")
schedules <- rbind(csci_spring_2019, csci_spring_2020, math_spring_2019, math_spring_2020)
```
